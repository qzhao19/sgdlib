#include <iostream>
#include <vector>
#include <gtest/gtest.h>
#include <gmock/gmock.h>

#include "sgdlib/algorithm/base.hpp"
#include "sgdlib/algorithm/sgd/scd.hpp"


class SCDTest : public ::testing::Test {
public:
    void SetUp() override {
        X_train = {
            -0.5557,0.25,-0.8643,-0.9165,-0.6665,-0.1666,-0.8643,-0.9165,-0.778,0.,-0.8984,-0.9165,-0.8335,-0.0833,-0.8306,-0.9165,-0.6113,
            0.3333,-0.8643,-0.9165,-0.389,0.5835,-0.7627,-0.75,-0.8335,0.1666,-0.8643,-0.8335,-0.6113,0.1666,-0.8306,-0.9165,-0.9443,-0.25,
            -0.8643,-0.9165,-0.6665,-0.0833,-0.8306,-1.,-0.389,0.4167,-0.8306,-0.9165,-0.722,0.1666,-0.7964,-0.9165,-0.722,-0.1666,-0.8643,
            -1.,-1.,-0.1666,-0.9663,-1.,-0.1666,0.6665,-0.932,-0.9165,-0.2222,1.,-0.8306,-0.75,-0.389,0.5835,-0.8984,-0.75,-0.5557,0.25,-0.8643,
            -0.8335,-0.2222,0.5,-0.7627,-0.8335,-0.5557,0.5,-0.8306,-0.8335,-0.389,0.1666,-0.7627,-0.9165,-0.5557,0.4167,-0.8306,-0.75,-0.8335,
            0.3333,-1.,-0.9165,-0.5557,0.0833,-0.7627,-0.6665,-0.722,0.1666,-0.695,-0.9165,-0.6113,-0.1666,-0.7964,-0.9165,-0.6113,0.1666,-0.7964,
            -0.75,-0.5,0.25,-0.8306,-0.9165,-0.5,0.1666,-0.8643,-0.9165,-0.778,0.,-0.7964,-0.9165,-0.722,-0.0833,-0.7964,-0.9165,-0.389,0.1666,
            -0.8306,-0.75,-0.5,0.75,-0.8306,-1.,-0.3333,0.8335,-0.8643,-0.9165,-0.6665,-0.0833,-0.8306,-0.9165,-0.6113,0.,-0.932,-0.9165,-0.3333,
            0.25,-0.8984,-0.9165,-0.6665,0.3333,-0.8643,-1.,-0.9443,-0.1666,-0.8984,-0.9165,-0.5557,0.1666,-0.8306,-0.9165,-0.6113,0.25,-0.8984,
            -0.8335,-0.8887,-0.75,-0.8984,-0.8335,-0.9443,0.,-0.8984,-0.9165,-0.6113,0.25,-0.7964,-0.5835,-0.5557,0.5,-0.695,-0.75,-0.722,-0.1666,
            -0.8643,-0.8335,-0.5557,0.5,-0.7964,-0.9165,-0.8335,0.,-0.8643,-0.9165,-0.4443,0.4167,-0.8306,-0.9165,-0.6113,0.0833,-0.8643,-0.9165,
            0.5,0.,0.2542,0.0833,0.1666,0.,0.1864,0.1666,0.4443,-0.0833,0.322,0.1666,-0.3333,-0.75,0.01695,0.,0.2222,-0.3333,0.2203,0.1666,-0.2222,
            -0.3333,0.1864,0.,0.1111,0.0833,0.2542,0.25,-0.6665,-0.6665,-0.2203,-0.25,0.2778,-0.25,0.2203,0.,-0.5,-0.4167,-0.01695,0.0833,-0.6113,-1.,
            -0.1526,-0.25,-0.1111,-0.1666,0.0847,0.1666,-0.05554,-0.8335,0.01695,-0.25,-0.,-0.25,0.2542,0.0833,-0.2778,-0.25,-0.11865,0.,0.3333,
            -0.0833,0.1526,0.0833,-0.2778,-0.1666,0.1864,0.1666,-0.1666,-0.4167,0.05084,-0.25,0.05554,-0.8335,0.1864,0.1666,-0.2778,-0.5835,-0.01695,
            -0.1666,-0.1111,0.,0.288,0.4167,-0.,-0.3333,0.01695,0.,0.1111,-0.5835,0.322,0.1666,-0.,-0.3333,0.2542,-0.0833,0.1666,-0.25,0.11865,0.,0.2778,
            -0.1666,0.1526,0.0833,0.389,-0.3333,0.288,0.0833,0.3333,-0.1666,0.356,0.3333,-0.05554,-0.25,0.1864,0.1666,-0.2222,-0.5,-0.1526,-0.25,-0.3333,
            -0.6665,-0.05084,-0.1666,-0.3333,-0.6665,-0.0847,-0.25,-0.1666,-0.4167,-0.01695,-0.0833,-0.05554,-0.4167,0.39,0.25,-0.389,-0.1666,0.1864,
            0.1666,-0.05554,0.1666,0.1864,0.25,0.3333,-0.0833,0.2542,0.1666,0.1111,-0.75,0.1526,0.,-0.2778,-0.1666,0.05084,0.,-0.3333,-0.5835,0.01695,
            0.,-0.3333,-0.5,0.1526,-0.0833,-0.,-0.1666,0.2203,0.0833,-0.1666,-0.5,0.01695,-0.0833,-0.6113,-0.75,-0.2203,-0.25,-0.2778,-0.4167,0.0847,0.,
            -0.2222,-0.1666,0.0847,-0.0833,-0.2222,-0.25,0.0847,0.,0.05554,-0.25,0.11865,0.,-0.5557,-0.5835,-0.322,-0.1666,-0.2222,-0.3333,0.05084,0.,
            0.1111,0.0833,0.695,1.,-0.1666,-0.4167,0.39,0.5,0.5557,-0.1666,0.661,0.6665,0.1111,-0.25,0.559,0.4167,0.2222,-0.1666,0.627,0.75,0.8335,
            -0.1666,0.8984,0.6665,-0.6665,-0.5835,0.1864,0.3333,0.6665,-0.25,0.7964,0.4167,0.3333,-0.5835,0.627,0.4167,0.6113,0.3333,0.729,1.,0.2222,
            0.,0.39,0.5835,0.1666,-0.4167,0.4575,0.5,0.389,-0.1666,0.5254,0.6665,-0.2222,-0.5835,0.356,0.5835,-0.1666,-0.3333,0.39,0.9165,0.1666,0.,
            0.4575,0.8335,0.2222,-0.1666,0.5254,0.4167,0.8887,0.5,0.932,0.75,0.8887,-0.5,1.,0.8335,-0.05554,-0.8335,0.356,0.1666,0.4443,0.,0.5933,
            0.8335,-0.2778,-0.3333,0.322,0.5835,0.8887,-0.3333,0.932,0.5835,0.1111,-0.4167,0.322,0.4167,0.3333,0.0833,0.5933,0.6665,0.6113,0.,0.695,
            0.4167,0.05554,-0.3333,0.288,0.4167,-0.,-0.1666,0.322,0.4167,0.1666,-0.3333,0.559,0.6665,0.6113,-0.1666,0.627,0.25,0.722,-0.3333,0.729,
            0.5,1.,0.5,0.8306,0.5835,0.1666,-0.3333,0.559,0.75,0.1111,-0.3333,0.39,0.1666,-0.,-0.5,0.559,0.0833,0.8887,-0.1666,0.729,0.8335,0.1111,
            0.1666,0.559,0.9165,0.1666,-0.0833,0.5254,0.4167,-0.05554,-0.1666,0.288,0.4167,0.4443,-0.0833,0.4915,0.6665,0.3333,-0.0833,0.559,0.9165,
            0.4443,-0.0833,0.39,0.8335,-0.1666,-0.4167,0.39,0.5,0.389,0.,0.661,0.8335,0.3333,0.0833,0.5933,1.,0.3333,-0.1666,0.4238,0.8335,0.1111,
            -0.5835,0.356,0.5,0.2222,-0.1666,0.4238,0.5835,0.05554,0.1666,0.4915,0.8335,-0.1111,-0.1666,0.39,0.4167,};
        y_train = {
            -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
            -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
            -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
            -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1};

        w0 = {1.0, 1.0, 1.0, 1.0};
        std::string loss = "LogLoss";
        double alpha = 0.01;
        double tol = 0.0001;
        std::size_t max_iters = 500;
        std::size_t random_seed = -1;
        bool shuffle = true;
        bool verbose = false;

        optimizer = std::make_unique<sgdlib::SCD>(w0,
            loss,
            alpha,
            tol,
            max_iters,
            random_seed,
            shuffle,
            verbose
        );
    }
    std::vector<double> X_train;
    std::vector<long> y_train;
    std::vector<double> w0;
    std::unique_ptr<sgdlib::BaseOptimizer> optimizer;
};

TEST_F(SCDTest, BasicOptimizationTest) {
    optimizer->optimize(X_train, y_train);
    const auto& w_opt = optimizer->get_weights();

    // check weight update
    EXPECT_EQ(w_opt.size(), 4);
    EXPECT_FALSE(w_opt[0] == w0[0]) << "Weights not updated";
    EXPECT_FALSE(w_opt[1] == w0[1]) << "Weights not updated";

    // print coefficients
    std::cout << "coefficients = ";
    for (auto w : w_opt) {
        std::cout << w << " ";
    }
    std::cout << std::endl;
}

TEST_F(SCDTest, ConvergenceTest) {
    EXPECT_NO_THROW(optimizer->optimize(X_train, y_train));
    EXPECT_TRUE(optimizer->get_weights().size() > 0);
}

TEST_F(SCDTest, ConvergenceSpeedTest) {
    std::vector<double> all_losses;
    all_losses.reserve(1000);
    optimizer->set_callback([&all_losses](const std::vector<double>& loss_history) {
        all_losses.insert(all_losses.end(), loss_history.begin(), loss_history.end());
    });
    optimizer->optimize(X_train, y_train);
    all_losses.shrink_to_fit();

    const double initial_loss = all_losses[0];
    const double final_loss = all_losses.back();
    const double improvement_ratio = (initial_loss - final_loss) / initial_loss;

    EXPECT_GT(improvement_ratio, 0.3f) << "insufficient convergence rate";

    // std::cout << "loss size = " << all_losses.size() << std::endl;
};
